# -*- coding: utf-8 -*-
"""price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZZb_q30ego8Mif8o12yMgN-gR9IhEys
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('TrainData.csv', na_values='?')

if 'ID' in df.columns:
    df.drop(columns=['ID'], inplace=True)

df.head()

cols_to_convert = ['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']

for col in cols_to_convert:
    df[col] = pd.to_numeric(df[col], errors='coerce')

df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)

df.info()

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

# Dictionary to save mappings for each column
label_mappings = {}

for col in df.select_dtypes(include='object').columns:
    df[col] = label_encoder.fit_transform(df[col])
    # Save mapping: number -> original label
    label_mappings[col] = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))

df.head()

print(label_mappings['body-style'])

plt.figure(figsize=(8, 5))
sns.lineplot(x='horsepower', y='price', data=df.sort_values('horsepower'), color='green')
plt.title("1. Horsepower vs Price")
plt.xlabel("Horsepower")
plt.ylabel("Price ($)")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Cars with higher horsepower consistently show higher prices. This suggests horsepower is a direct contributor to value, reflecting engine strength and performance. It helps predict that powerful vehicles are typically priced higher."""

df['engine-size'] = pd.to_numeric(df['engine-size'], errors='coerce')
df.dropna(subset=['engine-size', 'price'], inplace=True)
sorted_df = df.sort_values('engine-size').reset_index()

plt.figure(figsize=(8, 5))
plt.fill_between(sorted_df.index, sorted_df['engine-size'], alpha=0.5, label='Engine Size', color='orange')
plt.fill_between(sorted_df.index, sorted_df['price'], alpha=0.5, label='Price', color='blue')
plt.title("2. Engine Size vs Price")
plt.xlabel("Car Index (Sorted by Engine Size)")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Engine size and price rise together. Bigger engines are generally associated with more powerful and costly models. The trend confirms engine size is a key numeric feature in car pricing decisions."""

df['fuel-type'] = df['fuel-type'].replace({'gas': 'Gasoline', 'diesel': 'Diesel'})

plt.figure(figsize=(7, 5))
ax = sns.barplot(x='fuel-type', y='price', data=df, estimator='mean', palette='Set2')
plt.title("3. Average Price by Fuel Type\n (Gasoline = 0, Diesel = 1)")
plt.xlabel("Fuel Type")
plt.ylabel("Average Price ($)")

for p in ax.patches:
    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width()/2.5, p.get_height() + 200), ha='center')
plt.tight_layout()
plt.show()

"""Diesel vehicles are priced higher on average than gasoline ones. This reflects their typical use in high-efficiency or premium cars. Knowing the fuel type allows the model to estimate price tiers accurately."""

body_counts_encoded = df['body-style'].value_counts().sort_index()

body_style_labels = [label_mappings['body-style'][i] for i in body_counts_encoded.index]
sizes = body_counts_encoded.values
colors = sns.color_palette('pastel', len(body_style_labels))

fig, ax = plt.subplots(figsize=(7, 7))
wedges, texts, autotexts = ax.pie(
    sizes,
    autopct='%1.1f%%',
    labels=None,
    startangle=90,
    colors=colors,
    textprops=dict(color="black")
)

legend_labels = [f"{label} ({count})" for label, count in zip(body_style_labels, sizes)]
ax.legend(wedges, legend_labels, title="Body Style", loc="center left", bbox_to_anchor=(1, 0.5))

plt.title("Distribution of Car Body Styles")
plt.axis('equal')
plt.tight_layout()
plt.show()

"""Sedans and hatchbacks dominate the dataset, while convertibles and hardtops are rare. Body style helps predict which market segment a vehicle belongs to—compact, luxury, or sport—impacting its price range significantly."""

numeric_df = df.select_dtypes(include=['float64', 'int64']).dropna()
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("5. Correlation Matrix with Price and Key Features")
plt.tight_layout()
plt.show()

"""Price shows strong positive correlation with horsepower, engine size, curb weight, and width. It negatively correlates with city and highway mileage. This visualization guides feature selection for predictive modeling."""

correlation = df.corr()
target_corr = correlation['price'].drop('price')
selected_features = target_corr[abs(target_corr) > 0.3].index.tolist()

df = df[selected_features + ['price']]
print(df.head())

X = df.drop('price', axis=1)
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestRegressor(n_estimators=200, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred):.2f}")

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Car Prices')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.show()

!pip install joblib

import joblib
joblib.dump(model, 'car_price_model.pkl')

"""## **PREDICTION OF CAR PRICE FROM TEST DATA**"""

import pandas as pd
import joblib
from sklearn.preprocessing import LabelEncoder

# Step 1: Load the test data
df1 = pd.read_csv('TestData.csv', na_values='?')
if 'ID' in df1.columns:
    df1.drop(columns=['ID'], inplace=True)

# Step 2: Convert numeric columns
cols_to_convert1 = ['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm']
for col in cols_to_convert1:
    df1[col] = pd.to_numeric(df1[col], errors='coerce')

# Step 3: Fill missing numeric values with median (you can also use mean)
numeric_cols = df1.select_dtypes(include='number').columns
df1[numeric_cols] = df1[numeric_cols].fillna(df1[numeric_cols].median())

# Step 4: Encode object (categorical) columns using training mappings
for col in df1.select_dtypes(include='object').columns:
    if col in label_mappings:
        mapping = {v: k for k, v in label_mappings[col].items()}  # reverse original mapping
        df1[col] = df1[col].map(mapping).fillna(-1).astype(int)   # unseen labels as -1



plt.figure(figsize=(12, 10))
sns.heatmap(df1.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation Heatmap')
plt.show()


# Step 5: Ensure columns match what model expects (drop 'price' if present)

correlation = df.corr()
target_corr = correlation['price'].drop('price')
selected_features = target_corr[abs(target_corr) > 0.3].index.tolist()

df1_selected = df1[selected_features]

X_test_final = scaler.transform(df1_selected)

predicted_prices = model.predict(X_test_final)

submission_df = pd.DataFrame(predicted_prices, columns=['price'])
submission_df.to_csv('submission.csv', index=False, header=False)

